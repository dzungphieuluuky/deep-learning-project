import lightning as L
from torch.utils.data import DataLoader, TensorDataset
import torch
from transformers import AutoTokenizer

class LitDataModule(L.LightningDataModule):
    def __init__(self, batch_size: int = 32, num_workers: int = 4):
        super().__init__()
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
    
    def setup(self, stage: str = None):
        # Mock dataset
        texts = ["This is a sample text."] * 100
        labels = torch.randint(0, 2, (100,))
        
        encodings = self.tokenizer(texts, truncation=True, padding=True, return_tensors="pt")
        self.dataset = TensorDataset(encodings["input_ids"], encodings["attention_mask"], labels)
    
    def train_dataloader(self):
        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)
    
    def val_dataloader(self):
        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers)
    
    def test_dataloader(self):
        return DataLoader(self.dataset, batch_size=self.batch_size, num_workers=self.num_workers)